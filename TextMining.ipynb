{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec4fc47-7db9-497f-8bda-b71bc5c201db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jupit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     C:\\Users\\jupit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jupit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jupit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jupit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('webtext')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9825efbe-e25c-412a-8b48-52935c47e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', \"It's good to see you.\", \"Let's start our text mining class!\"]\n"
     ]
    }
   ],
   "source": [
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#주어진 텍스트를 문장 단위로 토큰화. 주로 . ! ? 등을 사용\n",
    "print(sent_tokenize(para))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976ed39b-455d-45eb-b7cd-4610f1ed73ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#주어진 text를 word 단위로 tokenize함\n",
    "print(word_tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c28b7f-2a9f-4346-8115-49c5ff98f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'It', \"'\", 's', 'good', 'to', 'see', 'you', '.', 'Let', \"'\", 's', 'start', 'our', 'text', 'mining', 'class', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(para))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd79ac-2f46-4005-8d2b-64db3d9dc816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It ' s로 구분하는지와 It 's로 구분하는지 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b39c60-7ac7-4e4e-8866-85701761c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\\\w 는 [a-zA-Z0-9_] 줄인 표현      [0-9], [a-zA-Z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "589949d8-2fbe-409d-8dd6-9ad4b5546a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', 'a', '7', 'b', '_', '5', 'c', '9', 'd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(\"[\\\\w]\", \"3a 7b_ '.^&5c9d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "789ad67e-bb9c-499d-9547-56929603082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'are', 'you', 'boy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#공백을 빼고 문자열 출력\n",
    "import re\n",
    "re.findall(\"[\\\\w]+\", \"How are you, boy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f04cc203-be7c-4840-b209-bb9e0132919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'oooo', 'oooo', 'ooo']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#o가 2~4회 반복된 문자열만 찾아내고 싶다면\n",
    "import re\n",
    "re.findall(\"[o]{2,4}\",\"oh, hoow are yoooou, boooooooy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c32564a-60db-46cb-a4fb-f483d21f8b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sorry', 'I', \"can't\", 'go', 'there']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "#regular expression(정규식)을 이용한 tokenizer\n",
    "#단어 단위로 tokenize \\\\w:문자나 숫자를 의미. 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아 냄\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "\n",
    "#can't를 하나의 단어로 인식\n",
    "print(tokenizer.tokenize(\"Sorry, I can't go there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81242e87-5857-4ce6-86fc-5d2e4ad9a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== English Stop Words ===\n",
      "{'t', 'here', 'whom', 'it', 'did', 'over', 'about', 'in', 'most', 'ours', 'themselves', 'as', 'only', 'hasn', 'at', 'ain', 'after', 'theirs', 'll', \"that'll\", 'now', \"you'll\", 'needn', \"wasn't\", \"doesn't\", 'for', 'this', 'into', 'is', 'had', 'how', 'be', 'what', 'by', 'wouldn', 'their', 'o', 'each', 'yourselves', 'not', 'has', 're', \"it's\", 'all', 'i', 'will', 'do', 'on', 'didn', 'so', 'before', 'doesn', \"hasn't\", \"wouldn't\", \"couldn't\", 'then', 'they', \"weren't\", 'and', 'haven', \"you've\", 'doing', 'further', 'a', 'through', 'but', 'other', 'again', 'once', 'am', 'or', 'that', 'being', 'ourselves', 'the', \"mustn't\", 'him', 'to', 'such', \"shan't\", \"aren't\", 'y', 'some', 'were', 'been', 'between', 'was', 'hers', 'below', 'her', 'couldn', 'myself', 'ma', 'our', 'there', 'you', 'because', 'who', 'those', 'against', 'd', 'itself', 'himself', 'having', \"haven't\", 'own', 'why', \"don't\", 'yours', 'same', 'are', 'of', \"hadn't\", 'until', 'too', 'them', 'under', 'your', \"needn't\", 'out', 'herself', 'don', 'off', \"didn't\", 'with', 'during', 'mustn', 'which', 'me', 's', 'while', 'just', \"won't\", 'when', 'm', \"you're\", 'yourself', 'both', 'above', 'he', 'wasn', 'mightn', 'we', 'does', 'very', 'if', 'down', 'should', 'no', 'hadn', 'shouldn', \"should've\", 'from', 'have', 'any', 'than', 'my', 'an', 'aren', 'isn', \"mightn't\", 'where', 'shan', 'weren', 'won', 'more', 'nor', \"isn't\", 'his', 'can', \"shouldn't\", \"you'd\", 'up', 'its', \"she's\", 'these', 've', 'she', 'few'}\n",
      "=== English Stop Words ===\n",
      "=== Token Words ===\n",
      "['sorry', 'i', \"couldn't\", 'go', 'to', 'movie', 'yesterday']\n",
      "=== Token Words ===\n",
      "=== Result Words ===\n",
      "['sorry', 'go', 'movie', 'yesterday']\n",
      "=== Result Words ===\n"
     ]
    }
   ],
   "source": [
    "#영어에서는 보통 길이가 3미만인 단어들은 삭제하는 것이 일반적이다 stopwords\n",
    "\n",
    "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
    "english_stops = set(stopwords.words('english')) #반복이 되지 않도록 set으로 변환\n",
    "\n",
    "print(\"=== English Stop Words ===\")\n",
    "print(english_stops)\n",
    "print(\"=== English Stop Words ===\")\n",
    "\n",
    "text1 = \"Sorry, I couldn't go to movie yesterday.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\\\w']+\")\n",
    "tokens = tokenizer.tokenize(text1.lower())   #word_tokenizer로 토큰화\n",
    "print(\"=== Token Words ===\")\n",
    "print(tokens)\n",
    "print(\"=== Token Words ===\")\n",
    "#stopwords를 제외한 단어들만으로 list를 생성\n",
    "result = [word for word in tokens if word not in english_stops]\n",
    "\n",
    "print(\"=== Result Words ===\")\n",
    "print(result)\n",
    "print(\"=== Result Words ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c88ba75b-2df1-4493-acc2-01ff4712706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cookeri cookbook\n"
     ]
    }
   ],
   "source": [
    "#어간 추출 - 포터 스테머\n",
    "#영어 사전에 없는 단어이지만 단어의 변화 규칙을 설명할 수 있다\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "932a6892-c45d-4797-97cd-5125c374d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tokens print ===\n",
      "['Hello', 'everyone', '.', 'It', \"'s\", 'good', 'to', 'see', 'you', '.', 'Let', \"'s\", 'start', 'our', 'text', 'mining', 'class', '!']\n",
      "=== Tokens print ===\n",
      "=== Result print ===\n",
      "['hello', 'everyon', '.', 'it', \"'s\", 'good', 'to', 'see', 'you', '.', 'let', \"'s\", 'start', 'our', 'text', 'mine', 'class', '!']\n",
      "=== Result print ===\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "para = \"Hello everyone. It's good to see you. Let's start our text mining class!\"\n",
    "tokens = word_tokenize(para)  #토큰화 실행\n",
    "\n",
    "print(\"=== Tokens print ===\")\n",
    "print(tokens)\n",
    "print(\"=== Tokens print ===\")\n",
    "\n",
    "result = [stemmer.stem(token) for token in tokens] #모든 토큰에 대해 스테밍 실행\n",
    "print(\"=== Result print ===\")\n",
    "print(result)\n",
    "print(\"=== Result print ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4013287-02a5-4460-903c-ea408b5d88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#어간 확인 - everyone -> everyon,  mining -> mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07ff7e61-3dc9-4477-8cb8-f16f0d26922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooking\n",
      "cook\n",
      "cookery\n",
      "cookbook\n"
     ]
    }
   ],
   "source": [
    "#표제어 추출(lemmatization)은 lemma로 변환한다는 뜻이고 lemma는 우리말로 단어의 기본형으로 번역된다.\n",
    "#표제어 추출은 단어의 기본형 추출\n",
    "#WordNetlemmatizer 사용\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cooking'))\n",
    "print(lemmatizer.lemmatize('cooking', pos='v'))  #품사를 지정 verb\n",
    "print(lemmatizer.lemmatize('cookery'))\n",
    "print(lemmatizer.lemmatize('cookbooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a9f947da-27d7-4af3-a6f6-b26146e277e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('everyone', 'NN'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('good', 'JJ'), ('to', 'TO'), ('see', 'VB'), ('you', 'PRP'), ('.', '.'), ('Let', 'VB'), (\"'s\", 'POS'), ('start', 'VB'), ('our', 'PRP$'), ('text', 'NN'), ('mining', 'NN'), ('class', 'NN'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"Hello everyone. It's good to see you. Let's start our text mining class!\")\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "43bee78b-7acc-46f1-9837-ff63dea7a172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eac310aa-2774-4a11-b847-57194a26c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everyone', 'good', 'see', 'Let', 'start', 'text', 'mining', 'class']\n"
     ]
    }
   ],
   "source": [
    "#원하는 품사의 단어들만 추출\n",
    "my_tag_set = ['NN', 'VB', 'JJ']\n",
    "my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]\n",
    "print(my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2c059be-1b59-49ca-b8d0-e3d36526379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b350d427-7f43-495d-9d12-b55d18c0a3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "03569981-1cdd-4055-88b8-759354472525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "549de698-0487-40f3-bf43-7351ba289a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello/NNP', 'everyone/NN', './.', 'It/PRP', \"'s/VBZ\", 'good/JJ', 'to/TO', 'see/VB', 'you/PRP', './.', 'Let/VB', \"'s/POS\", 'start/VB', 'our/PRP$', 'text/NN', 'mining/NN', 'class/NN', '!/.']\n"
     ]
    }
   ],
   "source": [
    "words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]\n",
    "print(words_with_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d939d2-6af9-4d9e-a53c-6b45c2a16165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
